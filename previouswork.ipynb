{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import threading\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse, unquote, urljoin\n",
    "\n",
    "# Get user input (defaulting to None if not entered)\n",
    "BASE_URL = input(\"Enter the URL: \").strip()\n",
    "\n",
    "start_year_input = input(\"Enter the start year (leave empty to start from the earliest available): \").strip()\n",
    "end_year_input = input(\"Enter the end year (leave empty to download up to the latest available): \").strip()\n",
    "\n",
    "# Convert inputs to integers if provided, otherwise None\n",
    "START_YEAR = int(start_year_input) if start_year_input.isdigit() else None\n",
    "END_YEAR = int(end_year_input) if end_year_input.isdigit() else None\n",
    "\n",
    "# Set up crawling queue\n",
    "urls_to_visit = [BASE_URL]\n",
    "visited_urls = set()\n",
    "max_year_encountered = None  # Track the latest year encountered\n",
    "\n",
    "# List to store batches of PDFs\n",
    "pdf_batch = []\n",
    "BATCH_SIZE = 10  # Process 10 PDFs at a time\n",
    "\n",
    "\n",
    "def find_pdf_links(url):\n",
    "    \"\"\"Find and return all PDF links on a webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        return [urljoin(BASE_URL, link['href']) for link in soup.find_all('a', href=True) if link['href'].endswith('.pdf')]\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error fetching {url}: {e}\")\n",
    "        return []\n",
    "\n",
    "\n",
    "def download_pdfs(pdf_list):\n",
    "    \"\"\"Download a batch of PDF files using threads.\"\"\"\n",
    "    threads = []\n",
    "    for pdf_url, paper_year in pdf_list:\n",
    "        thread = threading.Thread(target=download_pdf, args=(pdf_url, paper_year))\n",
    "        threads.append(thread)\n",
    "\n",
    "    # Start and join threads\n",
    "    for thread in threads:\n",
    "        thread.start()\n",
    "    for thread in threads:\n",
    "        thread.join()\n",
    "\n",
    "\n",
    "def download_pdf(pdf_url, paper_year):\n",
    "    \"\"\"Download a PDF file and save it under `paper/<year>/` directory.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(pdf_url, headers={'User-Agent': 'Mozilla/5.0'}, stream=True)\n",
    "        response.raise_for_status()\n",
    "\n",
    "        # Create directory for the paper year\n",
    "        save_directory = os.path.join(\"paper\", str(paper_year))\n",
    "        os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "        pdf_path = os.path.join(save_directory, extract_pdf_name(pdf_url))\n",
    "\n",
    "        with open(pdf_path, \"wb\") as pdf_file:\n",
    "            for chunk in response.iter_content(chunk_size=8192):\n",
    "                pdf_file.write(chunk)\n",
    "\n",
    "        print(f\"Downloaded: {pdf_path}\")\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Error downloading {pdf_url}: {e}\")\n",
    "\n",
    "\n",
    "def extract_pdf_name(pdf_url):\n",
    "    \"\"\"Extracts the filename from a PDF URL.\"\"\"\n",
    "    return unquote(urlparse(pdf_url).path.split('/')[-1])\n",
    "\n",
    "\n",
    "def extract_paper_year(pdf_url):\n",
    "    \"\"\"Extracts the year from the URL.\"\"\"\n",
    "    parsed_path = urlparse(pdf_url).path.split('/')\n",
    "\n",
    "    try:\n",
    "        year_index = parsed_path.index(\"paper\") + 1  # Year comes after 'paper'\n",
    "        return int(parsed_path[year_index])  # Convert to integer\n",
    "    except (ValueError, IndexError):\n",
    "        return None  # Return None if no valid year is found\n",
    "\n",
    "\n",
    "def crawl_website():\n",
    "    \"\"\"Crawl the website for PDF links and download them in batches.\"\"\"\n",
    "    global max_year_encountered\n",
    "\n",
    "    while urls_to_visit:\n",
    "        current_url = urls_to_visit.pop()\n",
    "        if current_url in visited_urls:\n",
    "            continue\n",
    "\n",
    "        visited_urls.add(current_url)\n",
    "        pdf_links = find_pdf_links(current_url)\n",
    "\n",
    "        if pdf_links:\n",
    "            for pdf in pdf_links:\n",
    "                paper_year = extract_paper_year(pdf)\n",
    "\n",
    "                # If year is found, apply filtering logic\n",
    "                if paper_year:\n",
    "                    if START_YEAR and paper_year < START_YEAR:\n",
    "                        print(f\"Skipping {pdf} (Year {paper_year} is before {START_YEAR})\")\n",
    "                        continue\n",
    "                    if END_YEAR and paper_year > END_YEAR:\n",
    "                        print(f\"Skipping {pdf} (Year {paper_year} is after {END_YEAR})\")\n",
    "                        continue\n",
    "\n",
    "                    # Track max encountered year and stop if it exceeds END_YEAR\n",
    "                    if max_year_encountered is None or paper_year > max_year_encountered:\n",
    "                        max_year_encountered = paper_year\n",
    "                    if END_YEAR and max_year_encountered > END_YEAR:\n",
    "                        print(f\"Stopping: Encountered year {max_year_encountered}, which exceeds END_YEAR {END_YEAR}\")\n",
    "                        return\n",
    "\n",
    "                    pdf_batch.append((pdf, paper_year))\n",
    "\n",
    "                    # Process PDFs in batches of 10\n",
    "                    if len(pdf_batch) >= BATCH_SIZE:\n",
    "                        download_pdfs(pdf_batch)\n",
    "                        pdf_batch.clear()\n",
    "\n",
    "        # Find new internal links to visit\n",
    "        try:\n",
    "            response = requests.get(current_url)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, \"html.parser\")\n",
    "\n",
    "            for link in soup.select(\"li a[href]\"):\n",
    "                new_url = urljoin(BASE_URL, link['href'])\n",
    "                if new_url.startswith(BASE_URL) and new_url not in visited_urls:\n",
    "                    urls_to_visit.append(new_url)\n",
    "        except requests.RequestException:\n",
    "            continue\n",
    "\n",
    "    # Download any remaining PDFs in the batch\n",
    "    if pdf_batch:\n",
    "        download_pdfs(pdf_batch)\n",
    "        pdf_batch.clear()\n",
    "\n",
    "\n",
    "# Run the crawler\n",
    "crawl_website()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import fitz #pymupdf\n",
    "import pandas as pd\n",
    "import ollama\n",
    "\n",
    "# Folder containing PDFs\n",
    "pdf_folder = \"paper\"\n",
    "output_file = \"pdf_metadata.xlsx\"\n",
    "\n",
    "# Function to extract metadata using Ollama Phi-3 Mini\n",
    "def extract_metadata(text):\n",
    "    prompt = f\"\"\"\n",
    "    Extract the following information from this research paper:\n",
    "    - Title\n",
    "    - Authors\n",
    "    - Year\n",
    "    - Country/Countries\n",
    "    - University\n",
    "    - 3 best-suited topic labels\n",
    "\n",
    "    If any information is missing, return 'Unknown'.\n",
    "\n",
    "    Text:\n",
    "    {text[:3000]}  # Limiting input to avoid long processing times\n",
    "    \"\"\"\n",
    "    response = ollama.chat(model=\"phi3:mini\", messages=[{\"role\": \"user\", \"content\": prompt}])\n",
    "    return response['message']['content']\n",
    "\n",
    "# Extract PDFs and get metadata\n",
    "data = []\n",
    "for root, _, files in os.walk(pdf_folder):\n",
    "    for file in files:\n",
    "        if file.endswith(\".pdf\"):\n",
    "            pdf_path = os.path.join(root, file)\n",
    "\n",
    "            try:\n",
    "                # Extract text from PDF\n",
    "                doc = fitz.open(pdf_path)\n",
    "                text = \"\\n\".join([page.get_text(\"text\") for page in doc])\n",
    "\n",
    "                # Get metadata from Ollama\n",
    "                metadata = extract_metadata(text)\n",
    "\n",
    "                # Parse metadata response (Ollama output should be structured)\n",
    "                lines = metadata.split(\"\\n\")\n",
    "                title = lines[0].replace(\"Title:\", \"\").strip() if \"Title:\" in lines[0] else \"Unknown\"\n",
    "                authors = lines[1].replace(\"Authors:\", \"\").strip() if len(lines) > 1 else \"Unknown\"\n",
    "                year = lines[2].replace(\"Year:\", \"\").strip() if len(lines) > 2 else \"Unknown\"\n",
    "                countries = lines[3].replace(\"Country/Countries:\", \"\").strip() if len(lines) > 3 else \"Unknown\"\n",
    "                university = lines[4].replace(\"University:\", \"\").strip() if len(lines) > 4 else \"Unknown\"\n",
    "                labels = lines[5].replace(\"Labels:\", \"\").strip() if len(lines) > 5 else \"Unknown\"\n",
    "\n",
    "                # Store data\n",
    "                data.append([pdf_path, title, authors, year, countries, university, labels])\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {file}: {e}\")\n",
    "\n",
    "# Save data to XLSX\n",
    "df = pd.DataFrame(data, columns=[\"PDF Path\", \"Title\", \"Authors\", \"Year\", \"Countries\", \"University\", \"Labels\"])\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Metadata saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
